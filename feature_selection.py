import argparse
import collections
import math

import numpy as np

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score

parser = argparse.ArgumentParser(
    description=
        """
        Out-of-core feature selection from an HDF file
        """
)


parser.add_argument(
    "input_file",
    default="pairwise_features.hdf",
    help="Input HDF5 file",
)

parser.add_argument(
    "--print-all-columns",
    help="Print all columns in HDF5 file before doing anything else",
    default=False,
    action="store_true",

)

parser.add_argument(
    "--iters",
    type=int,
    default=None,
    help="How many subset models to train (default: each feature in ~20 models)"
)

parser.add_argument(
    "--feature-fraction",
    type=float,
    default=0.1,
    help="What portion of features to use in each model"
)

parser.add_argument(
    "--sample-fraction",
    type=float,
    default=0.5,
    help="What portion of sample to train each model on"
)

parser.add_argument(
    "--target",
    default="Y",
    help="Name of target column"
)

parser.add_argument(
    "--ignore-columns",
    default="",
    help="Comma separated list of column names to ignore"
)

parser.add_argument(
    "--sample-attributes",
    default="",
    help="Comma separated list of columns to keep associated with each sample",
)

parser.add_argument(
    "--target-threshold",
    type = float,
    help="Threshold to turn continuous target into label categories"
)

parser.add_argument(
    "--model-fit-intercept",
    type = bool,
    default = True,
    help="Should trained models augment data with an intercept column?"
)

parser.add_argument(
    "--num-trees",
    type=int,
    default=21,
    help="Number of tree estimators to use with Random Forest models"
)

parser.add_argument(
    "--keep-feature-importance-ratio",
    default=0.001,
    type=float,
    help="Minimal ratio to the best feature importance"
)

parser.add_argument(
    "--output-data-file",
    type=str,
    default="selected.npz",
    help="Where to write selected features as an npz file"
)

parser.add_argument(
    "--use-pytables",
    help="Use PyTables instead of h5py to read HDF5 file",
    default=False,
    action="store_true",
)

parser.add_argument(
    "--balance-class-weights",
    help="Assign equal weight to pos/neg errors (for unbalanced data)",
    default=False,
    action="store_true"
)

parser.add_argument(
    "--downsample-majority-class",
    help="Number of samples per iteration limited by smallest class",
    default=False,
    action="store_true"
)

parser.add_argument(
    "--min-feature-variance",
    help="Minimum amount of variance in a feature",
    default=0.00000001,
    type=float,
)

parser.add_argument(
    "--feature-importance-cutoff",
    default=0.0001,
    type=float,
    help="Smallest feature importance to keep from a single model",
)

parser.add_argument(
    "--use-svm-models",
    default=False,
    action="store_true",
    help="In addition to LogisticRegression also use LinearSVC"
)



def examine_features(f, feature_names):
    bad_cols = set([])
    # checking features for NaN and infinite
    for i, feature_name in enumerate(sorted(feature_names)):
        x = f[feature_name][:]

        assert isinstance(x, np.ndarray), \
            "Invalid type for %s: %s" % (feature_name, type(x))
        # NumPy dtype codes:
        #
        # b  boolean
        # i  signed integer
        # u  unsigned integer
        # f  floating-point
        # c  comple x floating-point
        # O  object
        # S  (byte-)string
        # U  Unicode
        # V  void
        assert x.dtype.kind in 'biufc', \
            "Invalid feature type for '%s': %s" % (feature_name, x.dtype)

        nnz = (x!=0).sum()
        median = np.median(x)
        iqr = np.percentile(x, 75) - np.percentile(x, 25)
        std = np.std(x)
        print "Feature %d/%d: %s (nnz=%d/%d, median=%s, iqr=%s, std=%s)" % (
            i + 1,
            n_features,
            feature_name,
            nnz,
            len(x),
            median,
            iqr,
            std)

        if std < args.min_feature_variance:
            print "-- No variance!"
            bad_cols.add(feature_name)

        n_nan = np.isnan(x).sum()
        if n_nan > 0:
            print "-- # NaN: %d" % n_nan
            bad_cols.add(feature_name)

        n_inf = np.isinf(x).sum()
        if n_inf > 0:
            print "-- # inf: %d" % n_inf
            bad_cols.add(feature_name)
    return bad_cols

"""
Imitate the behavior of h5py for data generated by PyTables which
can't be loaded with h5py
"""
class PyTablesDataset(object):
    def __init__(self, field):
        self.field = field

    def __getitem__(self, arg):
        return self.field.read()[arg]

class PyTablesFile(object):
    def __init__(self, filename):
        import tables
        self.t = tables.open_file(filename)

    def __getitem__(self, name):
        return PyTablesDataset(getattr(self.t.root, name))

    def __contains__(self, name):
        return hasattr(self.t.root, name)

    def keys(self):
        return [subnode.name for subnode in self.t.get_node("/")]

    def iterkeys(self):
        return iter(self.keys())

if __name__ == "__main__":
    args = parser.parse_args()

    if args.use_pytables:
        import tables
        f = PyTablesFile(args.input_file)
    else:
        import h5py
        f = h5py.File(args.input_file)


    print "ARGUMENTS"
    print args

    if args.print_all_columns:
        print "Columns in %s" % args.input_file
        for k in f.iterkeys():
            print "  ", k

    assert args.sample_fraction > 0, \
        "Argument --sample-fraction must be positive"

    assert args.sample_fraction <= 0.5, \
        "Can't use more than 50% of the samples for training iterations, " \
        "not enough left for testing"

    assert 0 < args.feature_fraction <= 1.0, \
        "Argument --feature-fraction must be between 0.0 and 1.0"

    assert args.iters is None or args.iters > 0, \
        "Number of iterations (--iters) must be positive"

    assert not args.downsample_majority_class, \
        "Argument --downsample-majority-class not yet implemented"

    sample_attribute_names = [x for x in args.sample_attributes.split(",") if x]
    for attr_name in sample_attribute_names:
        assert attr_name in f, \
        "Attribute '%s' not found in %s" % (attr_name, args.input_file)

    target = args.target
    ignore_columns = [x for x in args.ignore_columns.split(",") if x]
    ignore_columns += sample_attribute_names
    ignore_columns += [target]

    assert target in f, \
        "Target column '%s' not found in %s" % (target, args.input_file)
    y = f[target][:]

    if args.target_threshold:
        y = y <= args.target_threshold
    else:
        unique_vals = np.unique(y)
        assert len(unique_vals) == 2, \
            "Expected binary label, use --target-threshold for float targets"

    feature_names = [
        name
        for name in f.iterkeys()
        if not name in ignore_columns
    ]

    n_samples = len(y)
    n_features = len(feature_names)
    n_samples_per_iter = int(n_samples * args.sample_fraction)
    n_features_per_iter = int(n_features * args.feature_fraction)

    bad_cols = examine_features(f, feature_names)

    if len(bad_cols) > 0:
        feature_names = [x for x in feature_names if x not in bad_cols]
        n_features = len(feature_names)
        print "Bad columns: %s" % bad_cols

    print "Samples per iter: %d / %d" % (n_samples_per_iter, n_samples)
    print "Features per iter: %d / %d" % (n_features_per_iter, n_features)

     # these will get shuffled to create subsets
    all_sample_indices = np.arange(n_samples)
    all_feature_indices = np.arange(n_features)

    feature_counts = collections.Counter()
    feature_nonzero = collections.Counter()

    # for each split try different hyperparameters and look
    # at non-zero coefficients in the model with best
    # predictive accuracy
    intercept = args.model_fit_intercept

    #
    class_weight = 'auto' if args.balance_class_weights else None
    Cs = [10, 1, 0.1, 0.01, 0.001]
    lr_models = [
        LogisticRegression(
            penalty='l1',
            C = c,
            fit_intercept=intercept,
            class_weight=class_weight)
        for c in  Cs
    ]

    n_estimators = args.num_trees
    rf_models = [
        RandomForestClassifier(
            n_estimators=n_estimators,
            max_depth=max_depth,
            criterion=c)
        for c in ["gini", "entropy"]
        for max_depth in [None, 10, 20]
    ]

    models = lr_models  + rf_models

    if args.use_svm_models:
        svm_models = [
            LinearSVC(
                penalty='l1',
                dual=False,
                C=c,
                fit_intercept=intercept,
                class_weight=class_weight)
            for c in Cs
        ]
        models += svm_models


    if args.iters is None:
        n_iters = int(math.ceil(20 * n_features / float(n_features_per_iter)))
    else:
        n_iters = args.iters

    for i in xrange(n_iters):
        np.random.shuffle(all_feature_indices)
        np.random.shuffle(all_sample_indices)
        feature_indices = all_feature_indices[:n_features_per_iter]

        training_indices = all_sample_indices[:n_samples_per_iter]
        testing_indices = \
            all_sample_indices[n_samples_per_iter:2*n_samples_per_iter]

        Y_train = y[training_indices]
        unique_training_labels = np.unique(Y_train)

        # TODO: implement downsample-majority-class
        #smallest_class_count = np.inf
        #smallest_class = None
        #class_counts = {}
        #for value in unique_training_labels:
        #    count = (Y_train==value).sum()
        #    class_counts[value] = count
        #    if count < smallest_class_count:
        #        smallest_class = value
        #        smallest_class_count = count

        Y_test = y[testing_indices]
        my = Y_test.mean()
        baseline_acc = max(my, 1.0 - my)
        baseline_auc = 0.5
        print
        print "============"
        print "Iter #%d/%d" % (i+1, n_iters)
        print "============"
        print
        print "-- Baseline accuracy for iter %0.4f" % baseline_acc
        X_train = []
        X_test = []
        for feature_idx in feature_indices:
            name = feature_names[feature_idx]
            col = f[name][:]
            train = col[training_indices]
            test = col[testing_indices]
            X_train.append(train)
            X_test.append(test)
        X_train = np.array(X_train, dtype=float).T
        X_test = np.array(X_test, dtype=float).T
        X_mean = X_train.mean(axis=0)
        X_train -= X_mean
        X_test -= X_mean
        X_std = X_train.std(axis=0)
        std_zero_mask = X_std < args.min_feature_variance
        std_zero_indices = np.nonzero(std_zero_mask)[0]
        # drop features with identical values across the sample
        if len(std_zero_indices) > 0:
            print "-- Dropping %d zero variance features: %s" % (
                len(std_zero_indices),
                [feature_names[global_idx]
                 for global_idx in
                   [feature_indices[local_idx]
                    for local_idx in std_zero_indices]
                ]
            )
            std_nonzero_mask = ~std_zero_mask
            X_train = X_train[:, std_nonzero_mask]
            X_test = X_test[:, std_nonzero_mask]
            X_std = X_std[std_nonzero_mask]
            feature_indices = [
                feature_indices[i]
                for i,b in enumerate(std_nonzero_mask)
                if b
            ]
            assert X_train.shape[1] == X_test.shape[1]
            assert len(X_std) == X_train.shape[1]
            assert len(feature_indices) == X_train.shape[1], \
                "%d != %d" % (len(feature_indices), X_train.shape[1])

        X_train /= X_std
        X_test /= X_std

        best_model = None
        best_accuracy = 0
        best_auc = 0

        print "-- train shape = %s, test shape = %s)" % (
            X_train.shape,
            X_test.shape,
        )
        for model in models:
            print
            print " *", model
            # for models that don't take a class balancing parameter
            # we have to manually reweight the samples by their inverse class
            # frequency
            use_sample_weights = (
                args.balance_class_weights and
                not hasattr(model, 'class_weight')
            )
            if use_sample_weights:
                sample_weights = np.zeros_like(Y_train)
                for class_value in unique_training_labels:
                    mask = Y_train == class_value
                    count = mask.sum()
                    weight = len(Y_train) / float(count)
                    print " -- sample weight for %d = %0.4f" % (
                        class_value, weight)
                    sample_weights[mask] = weight
                model.fit(X_train, Y_train, sample_weight = sample_weights)
            else:
                # either there's no class balancing or it's been
                # handled by the model's constructor
                model.fit(X_train, Y_train)

            pred = model.predict(X_test)
            accuracy = np.mean(pred == Y_test)

            # some classifier models come with a continuous 'decision_function',
            # for those that don't use the probability of the positive class
            if hasattr(model, 'decision_function'):
                prob = model.decision_function(X_test)
            else:
                prob = model.predict_proba(X_test)[:,-1]

            auc = roc_auc_score(Y_test, prob)
            print "  Accuracy=%0.4f, AUC=%0.4f" % (accuracy, auc)
            if auc > best_auc:
                best_model = model
                best_accuracy = accuracy
                best_auc = auc

        if hasattr(best_model, 'coef_'):
            coeff = best_model.coef_.ravel()
        else:
            coeff = best_model.feature_importances_

        abs_coeff = np.abs(coeff)
        nz_coeff_mask = abs_coeff > args.feature_importance_cutoff
        nnz_coeff = nz_coeff_mask.sum()
        prct_nz_coeff = nz_coeff_mask.mean()
        abs_coeff[~nz_coeff_mask] = 0
        print "-- %% nonzero coeffs %0.4f (min=%s, max=%s, median=%s)" % (
            prct_nz_coeff,
            np.min(coeff),
            np.max(coeff),
            np.median(coeff)
        )
        if prct_nz_coeff == 0:
            print "Skipping iteration %d due to all zero features" % (i+1)
            continue
        if best_auc < 0.5:
            print "Skipping iteration #%d due to low AUC: %0.4f" % (
                i+1, best_auc
            )
            continue

        # value of a predictor is how much better than baseline it did
        diff = (best_auc - 0.5)
        print "-- Improvement over baseline: %0.4f" % diff
        value = diff / baseline_auc
        total = abs_coeff.sum()
        fractions = abs_coeff / total
        for i, p in enumerate(fractions):
            feature_idx = feature_indices[i]
            name = feature_names[feature_idx]
            feature_counts[name] += 1
            feature_nonzero[name] += value * p

    feature_scores = collections.Counter()
    for name, v in feature_nonzero.iteritems():
        feature_scores[name] = v / float(feature_counts[name])

    n_zero_scores = sum(score == 0 for score in feature_scores.values())

    print
    print
    print "============================"
    print "# features with score = 0: %d/%d" % (
        n_zero_scores, len(feature_scores))
    print
    for name, score in feature_scores.most_common()[::-1]:
        print name, score, "(n=%d)" % feature_counts[name]

    n_nonzero_scores =  len(feature_scores) - n_zero_scores
    assert n_nonzero_scores > 0, "All features had zero importance!"

    feature_score_pairs = feature_scores.most_common(n_nonzero_scores)
    best_acc = feature_score_pairs[0][1]
    score_cutoff = best_acc * args.min_feature_importance_ratio
    keep = []
    for name, acc in feature_score_pairs:
        if acc >= score_cutoff:
            col = f[name][:]
            keep.append(col)
    print "---"
    print "# useful features: %d / %d" % (len(keep), n_features)

    X_kept = np.array(keep).T
    print "Final X.shape", X_kept.shape
    output_dictionary = {"X": X_kept, "y":y}
    for attr_name in sample_attribute_names:
        output_dictionary[attr_name] = f[attr_name][:]
    np.savez(args.output_data_file, **output_dictionary)